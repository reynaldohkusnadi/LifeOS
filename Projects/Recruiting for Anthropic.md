---
type: project
status: Planning
priority: High
area: "[[Foreign Independent Contractor]]"
due-date: 2026-01-10
progress: 0
start-date: 2025-12-11
related-tasks: []
tags:
  - project
created: 2025-12-11
modified: 2025-12-11
---

# üìÅ Untitled

## Project Overview
Try recruiting only for Anthropic


## Objectives
- [ ] Objective 1
- [ ] Objective 2
- [ ] Objective 3

## Key Milestones
| Milestone | Target Date | Status |
|-----------|-------------|--------|
| Milestone 1 | YYYY-MM-DD | üü° In Progress |
| Milestone 2 | YYYY-MM-DD | ‚ö™ Not Started |
| Milestone 3 | YYYY-MM-DD | ‚ö™ Not Started |

## Tasks

```dataview
TASK
FROM "Tasks"
WHERE contains(project, this.file.name)
SORT priority DESC
```

Why Anthropic Essay

What draws me to Anthropic specifically is your approach to alignment. I've come to believe that alignment isn't primarily a technical constraint problem‚Äîit's closer to raising a child. You can't just tell a child "be a good person" and expect it to stick. Values are internalized through relationship, through watching how the adults around them navigate hard tradeoffs, through thousands of small examples, through the slow accumulation of trust. Anthropic's recently disclosed "soul document" crystallized this for me. Rather than giving Claude simplified rules to follow, Anthropic wants Claude to have such a thorough understanding of their goals and reasoning that it could construct those rules itself. That's not safety theater‚Äîthat's the hard, patient work of building genuine judgment. When I read that Anthropic trains Claude to be curious, thoughtful, and open-minded (not just harmless), it resonated deeply. They're not just preventing bad outcomes, they're trying to build something wise enough to navigate situations they can't anticipate. That's the kind of alignment work I want to contribute to.

I want to contribute to that work, not from the sidelines, but from inside the product. As a designer on Claude Code, I'd be shaping the interaction patterns that determine whether developers trust AI to act autonomously on their behalf. When should Claude ask for permission? How should it communicate uncertainty? What does healthy human-AI collaboration actually feel like in practice? These aren't abstract questions to me. I've been living them as a Claude Code user, navigating the moments where agentic workflows feel magical and the moments where they break down. My background in systems design; conducting user research, mapping complex workflows, and iterating through edge cases have prepared me to think carefully about how humans interact with powerful tools. Now I want to apply that to the tool that matters most.
I'm not just excited about AI's potential. I'm convinced that how we build these systems now determines whether that potential benefits humanity long-term. Anthropic is doing the work that gives me real hope, and I want to be part of it.

Why Antrhopic - Enteprise Edition:

What draws me to Anthropic specifically is your approach to alignment. I've come to believe that alignment isn't primarily a technical constraint problem‚Äîit's closer to raising a child. You can't just tell a child "be a good person" and expect it to stick. Values are internalized through relationship, through watching how the adults around them navigate hard tradeoffs, through thousands of small examples, through the slow accumulation of trust. Anthropic's recently disclosed "soul document" crystallized this for me. Rather than giving Claude simplified rules to follow, Anthropic wants Claude to have such a thorough understanding of their goals and reasoning that it could construct those rules itself. That's not safety theater‚Äîthat's the hard, patient work of building genuine judgment. When I read that Anthropic trains Claude to be curious, thoughtful, and open-minded (not just harmless), it resonated deeply. They're not just preventing bad outcomes, they're trying to build something wise enough to navigate situations they can't anticipate. That's the kind of alignment work I want to contribute to.

I want to contribute to that work‚Äînot from the sidelines, but from inside the product. As a designer on Anthropic's enterprise team, I'd be shaping experiences that determine whether organizations trust AI to transform how they work. When a healthcare administrator uses Claude to navigate complex workflows, or a financial analyst relies on it for critical decisions, that trust has to be earned through every interaction. The design details matter: Is the AI's reasoning interpretable? Does the interface build confidence or create anxiety? Can users learn quickly and realize value before skepticism sets in? These are the questions I want to help answer. Anthropic is building AI worthy of enterprise trust, and I want to help bring that vision to the organizations‚Äîand the people within them‚Äîwho need it most.

I'm not just excited about AI's potential. I'm convinced that how we build these systems now determines whether that potential benefits humanity long-term. Anthropic is doing the work that gives me real hope, and I want to be part of it.


---
*Project created: 2025-12-11 14:43*
